{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# GLOBAL VARIABLES\n","WORKING_DIRECTORY = '/content/drive/MyDrive/epfl_ml_project'\n","DATASET_PATH = 'dataset/dataset.txt'\n","MODEL_MAX_INPUT_SIZE = 1024\n","MODEL_PATH = \"models/SpliceBERT.1024nt\"  # set the path to the folder of pre-trained SpliceBERT\n","SAMPLE_N_DATAPOINTS = 5000 # Sample a small subset of data for testing purposes\n","SEED = 66 # 42"],"metadata":{"id":"wxFr5OL5biAp","executionInfo":{"status":"ok","timestamp":1732795554527,"user_tz":-60,"elapsed":807,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"id":"NVhmg22WGimT","executionInfo":{"status":"ok","timestamp":1732795571821,"user_tz":-60,"elapsed":15137,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"outputs":[],"source":["# Install datasets as it is not already installed on colab\n","%%capture\n","!pip install datasets"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, AutoModelForTokenClassification\n","from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n","from datasets import Dataset\n","from sklearn.metrics import accuracy_score\n","import torch.nn.functional as F"],"metadata":{"id":"A7tjOYssGoxU","executionInfo":{"status":"ok","timestamp":1732790008473,"user_tz":-60,"elapsed":35505,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"-qWbBXP9GuIT","outputId":"4c5035bf-12cd-4af6-825e-bc2a0cf22665","executionInfo":{"status":"ok","timestamp":1732790008473,"user_tz":-60,"elapsed":8,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change working directory to Project folder, you may change this as needed\n","%cd {WORKING_DIRECTORY}\n","\n","from BP_LM.data_preprocessing import *"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gD-nhP5TGvSs","outputId":"4d2c5da1-9c44-4187-a4be-7001d52c0850","executionInfo":{"status":"ok","timestamp":1732795614525,"user_tz":-60,"elapsed":42706,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/epfl_ml_project\n"]}]},{"cell_type":"code","source":["# Load dataset\n","df = pd.read_csv(DATASET_PATH, sep='\\t')\n","\n","# Calculate BP_POS_WITHIN_STRAND\n","df['BP_POS_WITHIN_STRAND'] = df['IVS_SIZE'] + df['BP_ACC_DIST']\n","print(df.shape)\n"],"metadata":{"id":"pfstrSUpGyWh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732790056789,"user_tz":-60,"elapsed":22406,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}},"outputId":"950bb7c8-168c-4e54-ba47-6ad9d27d4ba7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(177980, 13)\n"]}]},{"cell_type":"code","source":["# Remove all data points where the BP is farther than\n","df = df[df['IVS_SIZE'] - df['BP_POS_WITHIN_STRAND'] <= MODEL_MAX_INPUT_SIZE]\n","print(df.shape)\n","\n","if SAMPLE_N_DATAPOINTS:\n","    df = df.sample(n = SAMPLE_N_DATAPOINTS, random_state=SEED)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HwouaHQaWwaD","executionInfo":{"status":"ok","timestamp":1732790056789,"user_tz":-60,"elapsed":10,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}},"outputId":"4e0a542b-d3c6-449b-b31f-471323666a35"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(177980, 13)\n"]}]},{"cell_type":"code","source":["# Create a split based on chromosome types (Alis idea)\n","train_chrs = [\"chr1\", \"chr2\", \"chr3\", \"chr4\",\n","              \"chr5\",\"chr10\",\n","              \"chr11\", \"chr12\", \"chr13\", \"chr14\",\n","              \"chr15\", \"chr16\", \"chr17\", \"chr18\",\n","              \"chr19\", \"chr22\",\n","              \"chrX\", \"chrY\"]\n","\n","# Keep chr6 and chr7 in train if we want a 90/10/10 train/val/test split\n","test_chrs = [\"chr8\", \"chr20\", \"chr6\"]\n","val_chrs = [\"chr9\", \"chr21\", \"chr7\"]\n","\n","train_df, test_df, val_df = split_train_test_on_chr(df, train_chrs, val_chrs, test_chrs, shuffle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DHRM1ZuwHFGk","outputId":"4a951c38-01ba-4cd5-948d-de52a59982a0","executionInfo":{"status":"ok","timestamp":1732790056789,"user_tz":-60,"elapsed":7,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Chromosomes in train set: {'chr2', 'chr19', 'chr11', 'chr5', 'chr12', 'chrY', 'chr14', 'chr15', 'chr17', 'chr1', 'chr13', 'chr18', 'chr22', 'chr10', 'chr3', 'chr4', 'chrX', 'chr16'}\n","Chromosomes in validation set: {'chr21', 'chr9', 'chr7'}\n","Chromosomes in test set: {'chr20', 'chr8', 'chr6'}\n","\n","Total data points: 1000\n","Train set contains 815 data points (81.50%)\n","Validation set contains 88 data points (8.80%)\n","Test set contains 97 data points (9.70%)\n"]}]},{"cell_type":"code","source":["train_seqs, train_labels = extract_intron_seq_and_labels(train_df, max_model_input_size=MODEL_MAX_INPUT_SIZE, truncate=True)\n","test_seqs, test_labels = extract_intron_seq_and_labels(test_df, max_model_input_size=MODEL_MAX_INPUT_SIZE, truncate=True)\n","val_seqs, val_labels = extract_intron_seq_and_labels(val_df, max_model_input_size=MODEL_MAX_INPUT_SIZE, truncate=True)"],"metadata":{"id":"Px1fK5CTLadf","executionInfo":{"status":"ok","timestamp":1732790056789,"user_tz":-60,"elapsed":5,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","\n","# finetuning SpliceBERT for token classification tasks\n","model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH, num_labels = 2) # We want binary classification on tokens so num_labels = 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oujkREPnSu4r","outputId":"011f6d61-0f76-4379-ac40-d96a6afd27f3","executionInfo":{"status":"ok","timestamp":1732790062465,"user_tz":-60,"elapsed":5681,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at models/SpliceBERT.1024nt and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["Now we do it for our data"],"metadata":{"id":"a2CdNhD_cQ0r"}},{"cell_type":"code","source":["# Tokenize the input data\n","train_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in train_seqs] # There shouldn't be any \"U\"s in the training data\n","test_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in test_seqs]\n","val_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in val_seqs]"],"metadata":{"id":"eA_N3LN1cSqw","executionInfo":{"status":"ok","timestamp":1732790062466,"user_tz":-60,"elapsed":4,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def pad_labels(labels, max_length, pad_label=-100):\n","    \"\"\"\n","    Pads labels with -100 which is apparenty standard in HuggingFace\n","    \"\"\"\n","    padded_labels = []\n","    for label in labels:\n","        if len(label) < max_length:\n","            padded = label + [pad_label] * (max_length - len(label))\n","        else:\n","            padded = label[:max_length]\n","        padded_labels.append(padded)\n","    return padded_labels\n","\n","max_length = MODEL_MAX_INPUT_SIZE # Ensure this matches the tokenizer's max_length\n","\n","train_labels_padded = pad_labels(train_labels, max_length)\n","test_labels_padded = pad_labels(test_labels, max_length)\n","val_labels_padded = pad_labels(val_labels, max_length)\n","\n","train_ids = tokenizer(train_seqs, padding='max_length', padding_side='left', max_length=max_length)\n","test_ids = tokenizer(test_seqs, padding='max_length', padding_side='left', max_length=max_length)\n","val_ids = tokenizer(val_seqs, padding='max_length', padding_side='left', max_length=max_length)\n","\n","# Create Datasets\n","train_dataset = Dataset.from_dict(train_ids)\n","train_dataset = train_dataset.add_column(\"labels\", train_labels_padded)\n","\n","test_dataset = Dataset.from_dict(test_ids)\n","test_dataset = test_dataset.add_column(\"labels\", test_labels_padded)\n","\n","val_dataset = Dataset.from_dict(val_ids)\n","val_dataset = val_dataset.add_column(\"labels\", val_labels_padded)\n","\n","# Set up the collator (I think it does padding)\n","data_collator = DataCollatorForTokenClassification(tokenizer)"],"metadata":{"id":"zT2kMfcmcvvr","executionInfo":{"status":"ok","timestamp":1732790067540,"user_tz":-60,"elapsed":5078,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Do not save to W&B\n","import os\n","os.environ[\"WANDB_MODE\"] = \"disabled\""],"metadata":{"id":"OeyBP_VdheGo","executionInfo":{"status":"ok","timestamp":1732790067540,"user_tz":-60,"elapsed":2,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","model_name = MODEL_PATH.split(\"/\")[-1]\n","batch_size = 8\n","\n","def compute_metrics(pred):\n","    predictions, labels = pred\n","    predictions = predictions[0]\n","\n","    predictions = np.array(predictions)\n","    labels = np.array(labels)\n","\n","    preds = np.argmax(predictions, axis=-1)\n","\n","    sequence_matches = 0\n","    total_sequences = 0\n","\n","    # for label, prediction in zip(labels, preds):\n","    #     nonpadded_indices = label != -100 # Only consider non-padded tokens\n","    #     nonpadded_labels = label[nonpadded_indices]\n","    #     preds = prediction[nonpadded_indices]\n","\n","\n","    #     if np.array_equal(nonpadded_labels, preds): # If the entire label matches, count it as correct\n","    #         sequence_matches += 1\n","\n","    #     total_sequences += 1\n","\n","    # acc = sequence_matches / total_sequences if total_sequences > 0 else 0\n","\n","    # return {\"accuracy\": acc}\n","\n","    # Variables for token-level metrics\n","    all_labels = []\n","    all_preds = []\n","\n","    # Iterate over each sequence\n","    for label, prediction in zip(labels, preds):\n","        # Ignore padded tokens\n","        nonpadded_indices = label != -100\n","        nonpadded_labels = label[nonpadded_indices]\n","        nonpadded_preds = prediction[nonpadded_indices]\n","\n","        # Sequence-level accuracy\n","        if np.array_equal(nonpadded_labels, nonpadded_preds):  # Entire sequence matches\n","            sequence_matches += 1\n","        total_sequences += 1\n","\n","        # Token-level metrics\n","        all_labels.extend(nonpadded_labels.tolist())\n","        all_preds.extend(nonpadded_preds.tolist())\n","\n","    # Sequence-level accuracy\n","    sequence_accuracy = sequence_matches / total_sequences if total_sequences > 0 else 0\n","\n","    # Token-level metrics\n","    precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n","    recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n","    f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=0)\n","\n","    return {\n","        \"sequence_accuracy\": sequence_accuracy,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1,\n","    }\n","\n","\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-secondary-structure\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=3e-4,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=5,\n","    weight_decay=0.001,\n","    metric_for_best_model=\"eval_sequence_accuracy\",\n","    load_best_model_at_end=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset, # SHOULD BE VAL DATASET\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()\n","\n","# Evaluate the model\n","evaluation_results = trainer.evaluate()\n","print(evaluation_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"snnpaLcpYD9o","outputId":"3e679fa2-0150-4021-f572-3cf12e40fe37","executionInfo":{"status":"ok","timestamp":1732790884649,"user_tz":-60,"elapsed":370260,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='510' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [510/510 06:04, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Sequence Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.009703</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.006390</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.009149</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>0.006545</td>\n","      <td>0.102273</td>\n","      <td>0.298013</td>\n","      <td>0.511364</td>\n","      <td>0.376569</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.008300</td>\n","      <td>0.006165</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11/11 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.006544713396579027, 'eval_sequence_accuracy': 0.10227272727272728, 'eval_precision': 0.2980132450331126, 'eval_recall': 0.5113636363636364, 'eval_f1': 0.37656903765690375, 'eval_runtime': 3.4891, 'eval_samples_per_second': 25.221, 'eval_steps_per_second': 3.153, 'epoch': 5.0}\n"]}]},{"cell_type":"code","source":["trained_model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}-finetuned-secondary-structure/checkpoint-2991\") #make sure you are loading the right checkpoint\n","trained_model = trained_model.to(device)"],"metadata":{"id":"3GnYHG9oYHdU","executionInfo":{"status":"aborted","timestamp":1732790133882,"user_tz":-60,"elapsed":3,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["showcase_seq = test_seqs[20]\n","showcase_ids = tokenizer.encode(showcase_seq) # N -> 5, A -> 6, C -> 7, G -> 8, T(U) -> 9. NOTE: a [CLS] and a [SEP] token will be added to the start and the end of seq\n","print(showcase_ids)\n","showcase_ids = torch.as_tensor(showcase_ids)\n","test_id = showcase_ids.unsqueeze(0)\n","test_id = test_id.to(device)\n","\n","test_logit = trained_model(test_id, output_hidden_states=False).logits\n","test_probs = torch.sigmoid(test_logit)\n","class1_probs = test_probs[..., 1]\n","max_indices = class1_probs.argmax(dim=-1)\n","predicted_classes = torch.zeros_like(class1_probs)\n","predicted_classes[torch.arange(test_logit.size(0)), max_indices] = 1\n","\n","predicted_classes = predicted_classes.squeeze(0)\n","\n","print(predicted_classes)\n","\n","print(sum(predicted_classes))\n","print(predicted_classes.argmax(dim = -1))\n","\n","print(torch.as_tensor(train_labels[20][predicted_classes.argmax(dim = -1)]))\n"],"metadata":{"id":"BqMdm25Lc2J3","executionInfo":{"status":"aborted","timestamp":1732790133882,"user_tz":-60,"elapsed":2,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_ml2avLRVD1p","executionInfo":{"status":"aborted","timestamp":1732790133882,"user_tz":-60,"elapsed":2,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"}}},"execution_count":null,"outputs":[]}]}