{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NVhmg22WGimT",
        "outputId": "94ac36aa-98c9-4b29-b5b2-bb0512c927d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install datasets as it is not already installed on colab\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, AutoModelForTokenClassification\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "A7tjOYssGoxU"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-qWbBXP9GuIT",
        "outputId": "fbec9498-ac59-40a3-e8be-9d07498aa87f"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change working directory to Project folder, you may change this as needed\n",
        "%cd \"/content/drive/MyDrive/Machine Learning (CS-433)/Project 2/BP_LM\"\n",
        "\n",
        "from data_preprocessing import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD-nhP5TGvSs",
        "outputId": "c42d59a3-8afa-41c2-d641-b0fd819826e4"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Machine Learning (CS-433)/Project 2/BP_LM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'dataset/dataset1.txt'\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "trunc_df = df[df['IVS_SIZE'] < 1000]\n",
        "df = trunc_df.sample(n=min(1000, len(trunc_df)))\n",
        "\n",
        "# Calculate BP_POS_WITHIN_STRAND\n",
        "df['BP_POS_WITHIN_STRAND'] = df['IVS_SIZE'] + df['BP_ACC_DIST']\n"
      ],
      "metadata": {
        "id": "pfstrSUpGyWh"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a split based on chromosome types (Alis idea)\n",
        "train_chrs = [\"chr1\", \"chr2\", \"chr3\", \"chr4\",\n",
        "              \"chr5\",\"chr10\",\n",
        "              \"chr11\", \"chr12\", \"chr13\", \"chr14\",\n",
        "              \"chr15\", \"chr16\", \"chr17\", \"chr18\",\n",
        "              \"chr19\", \"chr22\",\n",
        "              \"chrX\", \"chrY\"]\n",
        "\n",
        "# Keep chr6 and chr7 in train if we want a 90/10/10 train/val/test split\n",
        "test_chrs = [\"chr8\", \"chr20\", \"chr6\"]\n",
        "val_chrs = [\"chr9\", \"chr21\", \"chr7\"]\n",
        "\n",
        "train_df, test_df, val_df = split_train_test_on_chr(df, train_chrs, val_chrs, test_chrs, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHRM1ZuwHFGk",
        "outputId": "59dcc350-8113-462d-ec5e-1b5608dd68b2"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chromosomes in train set: {'chr3', 'chr15', 'chr4', 'chr22', 'chrX', 'chr2', 'chr5', 'chrY', 'chr19', 'chr16', 'chr18', 'chr17', 'chr14', 'chr11', 'chr13', 'chr10', 'chr12', 'chr1'}\n",
            "Chromosomes in validation set: {'chr9', 'chr21', 'chr7'}\n",
            "Chromosomes in test set: {'chr20', 'chr8', 'chr6'}\n",
            "\n",
            "Total data points: 1000\n",
            "Train set contains 826 data points (82.60%)\n",
            "Validation set contains 80 data points (8.00%)\n",
            "Test set contains 94 data points (9.40%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs, train_labels = extract_intron_seq_and_labels(train_df, max_model_input_size=1024, truncate=True)\n",
        "test_seqs, test_labels = extract_intron_seq_and_labels(test_df, max_model_input_size=1024, truncate=True)\n",
        "val_seqs, val_labels = extract_intron_seq_and_labels(val_df, max_model_input_size=1024, truncate=True)"
      ],
      "metadata": {
        "id": "Px1fK5CTLadf"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPLICEBERT_PATH = \"models/SpliceBERT.1024nt\"  # set the path to the folder of pre-trained SpliceBERT\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(SPLICEBERT_PATH)\n",
        "\n",
        "# finetuning SpliceBERT for token classification tasks\n",
        "model = AutoModelForTokenClassification.from_pretrained(SPLICEBERT_PATH, num_labels = 2) # We want binary classification on tokens so num_labels = 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oujkREPnSu4r",
        "outputId": "7b41a19d-bd27-4b32-cc0e-f39c4c0d4b14"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at models/SpliceBERT.1024nt and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we do it for our data"
      ],
      "metadata": {
        "id": "a2CdNhD_cQ0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input data\n",
        "train_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in train_seqs] #There shouldn't be any \"U\"s in the training data, but I have kept the replacer line\n",
        "test_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in test_seqs]\n",
        "val_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in val_seqs]"
      ],
      "metadata": {
        "id": "eA_N3LN1cSqw"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_labels(labels, max_length, pad_label=-100):\n",
        "    \"\"\"\n",
        "    Pads labels with -100 which is apparenty standard in HuggingFace\n",
        "    \"\"\"\n",
        "    padded_labels = []\n",
        "    for label in labels:\n",
        "        if len(label) < max_length:\n",
        "            padded = label + [pad_label] * (max_length - len(label))\n",
        "        else:\n",
        "            padded = label[:max_length]\n",
        "        padded_labels.append(padded)\n",
        "    return padded_labels\n",
        "\n",
        "max_length = 1024  # Ensure this matches the tokenizer's max_length\n",
        "\n",
        "train_labels_padded = pad_labels(train_labels, max_length)\n",
        "test_labels_padded = pad_labels(test_labels, max_length)\n",
        "val_labels_padded = pad_labels(val_labels, max_length)\n",
        "\n",
        "train_ids = tokenizer(train_seqs, padding='max_length', padding_side='left', max_length=max_length, truncation=True)\n",
        "test_ids = tokenizer(test_seqs, padding='max_length', padding_side='left', max_length=max_length, truncation=True)\n",
        "val_ids = tokenizer(val_seqs, padding='max_length', padding_side='left', max_length=max_length, truncation=True)\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = Dataset.from_dict(train_ids)\n",
        "train_dataset = train_dataset.add_column(\"labels\", train_labels_padded)\n",
        "\n",
        "test_dataset = Dataset.from_dict(test_ids)\n",
        "test_dataset = test_dataset.add_column(\"labels\", test_labels_padded)\n",
        "\n",
        "val_dataset = Dataset.from_dict(val_ids)\n",
        "val_dataset = val_dataset.add_column(\"labels\", val_labels_padded)\n",
        "\n",
        "# Set up the collator (I think it does padding)\n",
        "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zT2kMfcmcvvr"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = SPLICEBERT_PATH.split(\"/\")[-1]\n",
        "batch_size = 8\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    predictions, labels = pred\n",
        "    predictions = predictions[0]\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    preds = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    sequence_matches = 0\n",
        "    total_sequences = 0\n",
        "\n",
        "    for label, prediction in zip(labels, preds):\n",
        "        nonpadded_indices = label != -100 # Only consider non-padded tokens\n",
        "        nonpadded_labels = label[nonpadded_indices]\n",
        "        preds = prediction[nonpadded_indices]\n",
        "\n",
        "\n",
        "        if np.array_equal(nonpadded_labels, preds): # If the entire label matches, count it as correct\n",
        "            sequence_matches += 1\n",
        "\n",
        "        total_sequences += 1\n",
        "\n",
        "    acc = sequence_matches / total_sequences if total_sequences > 0 else 0\n",
        "\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-secondary-structure\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.001,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluation_results = trainer.evaluate()\n",
        "print(evaluation_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "snnpaLcpYD9o",
        "outputId": "a39151de-ad13-44da-9db8-94c14275512a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/312 02:17 < 01:01, 1.56 it/s, Epoch 2.07/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.016152</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.016158</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}-finetuned-secondary-structure/checkpoint-2991\") #make sure you are loading the right checkpoint\n",
        "trained_model = trained_model.to(device)"
      ],
      "metadata": {
        "id": "3GnYHG9oYHdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "showcase_seq = test_seqs[20]\n",
        "showcase_ids = tokenizer.encode(showcase_seq) # N -> 5, A -> 6, C -> 7, G -> 8, T(U) -> 9. NOTE: a [CLS] and a [SEP] token will be added to the start and the end of seq\n",
        "print(showcase_ids)\n",
        "showcase_ids = torch.as_tensor(showcase_ids)\n",
        "test_id = showcase_ids.unsqueeze(0)\n",
        "test_id = test_id.to(device)\n",
        "\n",
        "test_logit = trained_model(test_id, output_hidden_states=False).logits\n",
        "test_probs = torch.sigmoid(test_logit)\n",
        "class1_probs = test_probs[..., 1]\n",
        "max_indices = class1_probs.argmax(dim=-1)\n",
        "predicted_classes = torch.zeros_like(class1_probs)\n",
        "predicted_classes[torch.arange(test_logit.size(0)), max_indices] = 1\n",
        "\n",
        "predicted_classes = predicted_classes.squeeze(0)\n",
        "\n",
        "print(predicted_classes)\n",
        "\n",
        "print(sum(predicted_classes))\n",
        "print(predicted_classes.argmax(dim = -1))\n",
        "\n",
        "print(torch.as_tensor(train_labels[20][predicted_classes.argmax(dim = -1)]))\n"
      ],
      "metadata": {
        "id": "BqMdm25Lc2J3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "de4f725e-ae77-4034-d1ca-cd5629ac37d1"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 9, 7, 6, 8, 6, 6, 7, 9, 6, 7, 6, 8, 8, 9, 8, 6, 6, 6, 9, 9, 6, 7, 6, 8, 6, 8, 9, 6, 9, 6, 9, 9, 9, 6, 9, 6, 6, 6, 9, 7, 6, 9, 9, 7, 6, 9, 9, 6, 6, 6, 9, 9, 8, 7, 6, 8, 6, 9, 7, 9, 6, 6, 9, 6, 9, 6, 7, 6, 8, 6, 9, 7, 6, 6, 6, 9, 9, 9, 7, 9, 6, 7, 6, 6, 8, 9, 8, 9, 7, 6, 9, 6, 8, 7, 6, 8, 9, 8, 6, 6, 9, 9, 9, 9, 8, 9, 6, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 7, 7, 6, 9, 7, 7, 7, 9, 7, 9, 9, 8, 6, 8, 8, 9, 9, 9, 9, 8, 9, 8, 6, 7, 7, 7, 6, 9, 9, 9, 9, 7, 6, 9, 7, 6, 8, 6, 6, 6, 9, 6, 8, 9, 8, 6, 8, 9, 9, 8, 7, 9, 9, 7, 6, 8, 7, 6, 8, 9, 8, 6, 9, 9, 6, 9, 7, 6, 8, 6, 9, 9, 8, 8, 6, 8, 6, 8, 8, 6, 8, 6, 6, 7, 9, 8, 7, 7, 7, 7, 7, 6, 7, 7, 7, 9, 7, 6, 9, 8, 7, 7, 9, 7, 6, 9, 9, 7, 7, 6, 8, 6, 8, 6, 6, 9, 8, 9, 6, 9, 7, 6, 9, 6, 9, 8, 9, 9, 7, 9, 8, 8, 8, 8, 8, 6, 8, 9, 8, 7, 6, 9, 8, 9, 6, 6, 9, 9, 9, 8, 6, 6, 6, 6, 6, 8, 6, 9, 8, 7, 7, 9, 7, 8, 8, 9, 9, 6, 9, 9, 7, 9, 8, 6, 9, 6, 7, 7, 6, 8, 8, 7, 6, 9, 9, 9, 7, 9, 8, 7, 7, 9, 7, 7, 6, 9, 7, 9, 7, 9, 7, 6, 6, 8, 8, 6, 6, 7, 7, 6, 9, 6, 9, 7, 7, 8, 7, 9, 9, 9, 6, 7, 9, 6, 7, 9, 8, 8, 8, 9, 9, 8, 6, 8, 6, 9, 6, 9, 6, 7, 9, 7, 9, 7, 6, 8, 9, 8, 8, 6, 6, 9, 9, 6, 6, 6, 9, 8, 8, 9, 9, 9, 8, 9, 6, 7, 6, 7, 6, 6, 9, 9, 6, 6, 6, 7, 7, 6, 9, 9, 9, 6, 9, 7, 7, 9, 9, 6, 6, 8, 6, 8, 6, 6, 9, 6, 8, 9, 7, 6, 8, 8, 7, 9, 9, 9, 7, 9, 8, 6, 7, 9, 8, 7, 7, 6, 9, 8, 7, 7, 9, 9, 7, 9, 7, 9, 9, 8, 9, 9, 9, 9, 7, 9, 9, 8, 6, 7, 7, 6, 6, 6, 7, 9, 9, 9, 9, 9, 9, 6, 9, 9, 7, 9, 6, 9, 8, 9, 8, 7, 9, 6, 6, 8, 8, 6, 7, 6, 8, 6, 6, 6, 8, 9, 6, 6, 7, 6, 6, 6, 9, 8, 9, 9, 7, 6, 9, 9, 6, 7, 9, 6, 6, 7, 6, 7, 6, 7, 9, 6, 9, 9, 8, 6, 8, 8, 8, 6, 8, 6, 6, 9, 6, 6, 8, 6, 9, 9, 8, 9, 7, 6, 8, 9, 6, 8, 6, 6, 9, 8, 8, 8, 7, 6, 7, 9, 7, 7, 7, 6, 8, 9, 9, 8, 9, 9, 6, 6, 8, 7, 6, 9, 7, 6, 9, 9, 7, 6, 9, 8, 7, 6, 7, 7, 6, 6, 6, 7, 6, 7, 9, 8, 7, 9, 7, 9, 6, 6, 6, 9, 8, 9, 9, 6, 7, 6, 8, 6, 7, 6, 6, 7, 9, 9, 9, 6, 9, 9, 7, 9, 9, 7, 6, 7, 7, 6, 6, 9, 8, 9, 6, 8, 8, 6, 9, 6, 8, 7, 7, 9, 9, 9, 6, 9, 9, 6, 7, 7, 9, 8, 7, 6, 9, 6, 9, 9, 6, 9, 6, 8, 6, 7, 6, 6, 8, 8, 8, 6, 6, 6, 7, 6, 6, 7, 9, 9, 7, 6, 9, 8, 7, 6, 8, 8, 7, 6, 6, 9, 9, 6, 7, 7, 9, 7, 9, 7, 7, 6, 6, 8, 8, 6, 7, 6, 9, 7, 6, 6, 6, 9, 6, 6, 6, 9, 9, 7, 6, 7, 6, 8, 6, 6, 6, 6, 8, 7, 9, 8, 9, 6, 9, 9, 7, 6, 6, 6, 9, 6, 7, 6, 8, 8, 7, 9, 9, 9, 8, 9, 7, 6, 6, 6, 9, 7, 7, 7, 6, 6, 8, 7, 6, 6, 7, 6, 6, 9, 7, 9, 9, 8, 6, 9, 9, 9, 6, 8, 6, 8, 6, 9, 8, 8, 7, 9, 9, 9, 8, 8, 6, 8, 7, 7, 6, 8, 7, 8, 8, 7, 7, 9, 8, 6, 8, 9, 9, 7, 6, 6, 6, 9, 8, 7, 9, 8, 8, 9, 9, 7, 9, 8, 7, 9, 8, 9, 9, 9, 7, 6, 7, 9, 6, 8, 7, 7, 6, 8, 8, 6, 6, 6, 7, 9, 9, 8, 7, 6, 8, 7, 6, 6, 8, 7, 9, 6, 7, 7, 6, 6, 6, 7, 7, 9, 7, 9, 7, 9, 7, 9, 9, 7, 9, 7, 6, 8, 9, 9, 9, 9, 7, 9, 7, 6, 9, 9, 9, 8, 9, 6, 6, 6, 6, 9, 6, 6, 7, 8, 9, 9, 6, 8, 7, 8, 6, 9, 8, 6, 7, 6, 8, 9, 6, 8, 6, 8, 6, 9, 6, 7, 9, 6, 6, 9, 6, 9, 6, 7, 7, 9, 7, 6, 9, 8, 9, 6, 8, 9, 9, 8, 6, 9, 6, 8, 8, 6, 6, 8, 6, 9, 9, 6, 6, 6, 9, 6, 6, 9, 6, 7, 6, 9, 8, 9, 6, 6, 6, 8, 7, 6, 9, 9, 8, 6, 8, 6, 6, 8, 6, 8, 9, 8, 9, 7, 9, 8, 6, 7, 6, 7, 6, 9, 8, 8, 9, 6, 8, 8, 6, 6, 7, 6, 6, 6, 6, 9, 6, 6, 7, 9, 9, 7, 9, 9, 7, 9, 9, 9, 9, 9, 9, 9, 6, 6, 9, 7, 6, 8, 3]\n",
            "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "tensor(1., device='cuda:0')\n",
            "tensor(1003, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-140-f1c285c92b00>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ml2avLRVD1p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}