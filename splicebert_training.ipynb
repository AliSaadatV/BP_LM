{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install datasets as it is not already installed on colab\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cCX_QQAJuHn",
        "outputId": "76b9ff8b-9a85-4518-99fe-523ecbac835c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup colab and load scripts from github"
      ],
      "metadata": {
        "id": "x8VPCc7SdQGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change working directory to Project folder, you may change this as needed\n",
        "%cd \"/content/drive/MyDrive/Machine_Learning_(CS-433)/Project_2\"\n",
        "\n",
        "#!git clone https://github.com/AliSaadatV/BP_LM.git clone repository if you haven't already\n",
        "\n",
        "#Import useful functions from repository\n",
        "from BP_LM.data_preprocessing import preprocess_data, split_train_test_on_chr,  extract_intron_seq_and_labels, truncate_strands"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBJNtW_XdceS",
        "outputId": "68fb3911-8bc4-4f36-db28-5715bee3a3d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Machine_Learning_(CS-433)/Project_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the dataset and prepare the labels and sequences"
      ],
      "metadata": {
        "id": "wNlaTgOYWpj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# I assume this directory contains two folders: data and models, containing the data and models respectively.\n",
        "file_path = 'data/dataset.txt'\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "preprocess_data(df)\n",
        "\n",
        "# Drop sequences that don't natively fit in splicebert\n",
        "df = df.drop(df[df[\"IVS_SIZE\"] > 1024].index)\n",
        "# This should be improved at a later time\n",
        "\n",
        "#Pick just a random subset to make the processing to come run fast.\n",
        "#If you want to train for real then remove this line.\n",
        "df = df.sample(n = 1000)\n",
        "\n",
        "#Create a split based on chromosome types (Alis idea)\n",
        "train_chrs = [\"chr1\", \"chr2\", \"chr3\", \"chr4\",\n",
        "              \"chr5\", \"chr6\", \"chr7\",\"chr10\",\n",
        "              \"chr11\", \"chr12\", \"chr13\", \"chr14\",\n",
        "              \"chr15\", \"chr16\", \"chr17\", \"chr18\",\n",
        "              \"chr19\", \"chr20\", \"chr21\", \"chr22\",\n",
        "              \"chrX\", \"chrY\"]\n",
        "\n",
        "test_chrs = [\"chr8\"]\n",
        "val_chrs = [\"chr9\"]\n",
        "\n",
        "train_df, test_df, val_df = split_train_test_on_chr(df, train_chrs, val_chrs, test_chrs, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7WNvUzx_bH6",
        "outputId": "b9821f00-c7dd-4683-eb80-cd7f5ba460a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/Machine_Learning_(CS-433)/Project_2/BP_LM/data_preprocessing.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['STRAND'] = df['STRAND'].replace({'+': 1, '-': 0})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chromosomes in train set: {'chr22', 'chr2', 'chr17', 'chr5', 'chr10', 'chr21', 'chr6', 'chr4', 'chr15', 'chr7', 'chr3', 'chr13', 'chr16', 'chr11', 'chr19', 'chr1', 'chrX', 'chr12', 'chrY', 'chr14', 'chr20', 'chr18'}\n",
            "Chromosomes in validation set: {'chr9'}\n",
            "Chromosomes in test set: {'chr8'}\n",
            "\n",
            "Total data points: 1000\n",
            "Train set contains 940 data points (94.00%)\n",
            "Validation set contains 37 data points (3.70%)\n",
            "Test set contains 23 data points (2.30%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs, train_labels = extract_intron_seq_and_labels(train_df, max_model_input_size=1024, truncate=False) # I don't trust the truncation function yet.\n",
        "test_seqs, test_labels = extract_intron_seq_and_labels(test_df, max_model_input_size=1024, truncate=False)\n",
        "val_seqs, val_labels = extract_intron_seq_and_labels(val_df, max_model_input_size=1024, truncate=False)"
      ],
      "metadata": {
        "id": "rjO8wJHqgTRr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzYS6GWgRR1D",
        "outputId": "8475c159-2168-47b5-dc30-b464066efcd6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CHR', 'START', 'END', 'STRAND', 'GENE', 'TRANSCRIPT', 'IVS',\n",
              "       'IVS_SIZE', 'BP_POS', 'BP_ACC_DIST', 'BP_ACC_SEQ', 'IVS_SEQ',\n",
              "       'BP_POS_WITHIN_STRAND'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the splicebert model and tokenizer"
      ],
      "metadata": {
        "id": "Xzn3XsqAWx_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I-5T13rQAbi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b091a27-315f-4fb9-a90f-9f5d310b6861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at models/SpliceBERT.1024nt and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "SPLICEBERT_PATH = \"models/SpliceBERT.1024nt\"  # set the path to the folder of pre-trained SpliceBERT\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, AutoModelForTokenClassification\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(SPLICEBERT_PATH)\n",
        "\n",
        "# finetuning SpliceBERT for token classification tasks\n",
        "model = AutoModelForTokenClassification.from_pretrained(SPLICEBERT_PATH, num_labels = 2) # We want binary classification on tokens so num_labels = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show a case of inference"
      ],
      "metadata": {
        "id": "so95_lpXW3al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare a test input to show how to do inference\n",
        "showcase_seq = \"ACGUACGuacguaCGuGCAUGUCAUGC\"  ## WARNING: this is just a demo. SpliceBERT may not work on sequences shorter than 64nt as it was trained on sequences of 64-1024nt in length\n",
        "showcase_seq = ' '.join(list(showcase_seq.upper().replace(\"U\", \"T\"))) # U -> T and add whitespace\n",
        "showcase_ids = tokenizer.encode(showcase_seq) # N -> 5, A -> 6, C -> 7, G -> 8, T(U) -> 9. NOTE: a [CLS] and a [SEP] token will be added to the start and the end of seq\n",
        "print(showcase_ids) #Print the tokenized version\n",
        "showcase_ids = torch.as_tensor(showcase_ids) # convert python list to Tensor\n",
        "showcase_ids = showcase_ids.unsqueeze(0) # add batch dimension, shape: (batch_size, sequence_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL8Vked5UYkx",
        "outputId": "3992dfa3-c1f0-4e88-feef-7ff37a78f02f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 8, 7, 6, 9, 8, 9, 7, 6, 9, 8, 7, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#infer with the (untrained) model\n",
        "showcase_logit = model(showcase_ids, output_hidden_states=False).logits"
      ],
      "metadata": {
        "id": "9Dd512BZUeQc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XvYheoTEN9Zw"
      },
      "outputs": [],
      "source": [
        "#Define a function for making the output easily interpretable\n",
        "def logittobinary(logits):\n",
        "  \"\"\"\n",
        "  This is just a ghetto function for converting binary logits into binary strings\n",
        "  There might be an existing method for this in transformers, but I coulnd't find any\n",
        "  so i wrote this real quick\n",
        "  \"\"\"\n",
        "  output_list = []\n",
        "  for j in range(len(logits)):\n",
        "    binary = \"\"\n",
        "    for i in range(len(logits[j,:,1])):\n",
        "      if logits[j,i,0] < logits[j,i,1]:\n",
        "        binary += \"1\"\n",
        "      else:\n",
        "        binary += \"0\"\n",
        "    output_list.append(binary)\n",
        "  return output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8zvPbMoGuFK",
        "outputId": "72460a2c-d582-4414-e5e4-36fe623ed724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['11011101110111011101111111111']\n"
          ]
        }
      ],
      "source": [
        "print(logittobinary(showcase_logit))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is not yet finetuned so this output is just gibberish at the moment. Notice that for each token the likelyhood of a branchpoint is about the same as not a branchpoint (depends a bit on seed), which should definately not be the case once the model has been trained."
      ],
      "metadata": {
        "id": "LZ5tDwQxVArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the training data and put it into the dataset format"
      ],
      "metadata": {
        "id": "OCKbfRHsXHc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RkPF0Xq6AfLt"
      },
      "outputs": [],
      "source": [
        "#Tokenize the input data\n",
        "train_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in train_seqs] #There shouldn't be any \"U\"s in the training data, but I have kept the replacer line\n",
        "test_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in test_seqs]\n",
        "val_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in val_seqs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7-EHbIu2AjhQ"
      },
      "outputs": [],
      "source": [
        "#package for use in dataset class\n",
        "train_ids = tokenizer(train_seqs, padding = 'max_length', padding_side = 'left', max_length = 1024)\n",
        "test_ids = tokenizer(test_seqs, padding = 'max_length', padding_side = 'left', max_length = 1024)\n",
        "val_ids = tokenizer(val_seqs, padding = 'max_length', padding_side = 'left', max_length = 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2Q3ihysxI7Q6"
      },
      "outputs": [],
      "source": [
        "# Build the dataset structure we will pass for training\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_ids)\n",
        "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
        "\n",
        "test_dataset = Dataset.from_dict(test_ids)\n",
        "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
        "\n",
        "val_dataset = Dataset.from_dict(val_ids)\n",
        "val_dataset = val_dataset.add_column(\"labels\", val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dI4LDqvNIT-m"
      },
      "outputs": [],
      "source": [
        "#Set up the collator (I think it does padding)\n",
        "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I6zO7lLZI9wF"
      },
      "outputs": [],
      "source": [
        "#Define model training parameters\n",
        "model_name = SPLICEBERT_PATH.split(\"/\")[-1]\n",
        "batch_size = 4\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-secondary-structure\",\n",
        "    eval_strategy = \"no\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.001,\n",
        "    #load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    #push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "MOEm66ctT8dN",
        "outputId": "85d60192-a02f-406f-b21a-7ef50b6d9a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-ffb7fee7e86c>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moliversmedt\u001b[0m (\u001b[33moliversmedt-epfl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Machine_Learning_(CS-433)/Project_2/wandb/run-20241123_113042-tg8lp3hi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oliversmedt-epfl/huggingface/runs/tg8lp3hi' target=\"_blank\">SpliceBERT.1024nt-finetuned-secondary-structure</a></strong> to <a href='https://wandb.ai/oliversmedt-epfl/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/oliversmedt-epfl/huggingface' target=\"_blank\">https://wandb.ai/oliversmedt-epfl/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/oliversmedt-epfl/huggingface/runs/tg8lp3hi' target=\"_blank\">https://wandb.ai/oliversmedt-epfl/huggingface/runs/tg8lp3hi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 95/235 54:01 < 1:21:19, 0.03 it/s, Epoch 0.40/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "osaAl_hGLj56"
      },
      "outputs": [],
      "source": [
        "trained_model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}-finetuned-secondary-structure/checkpoint-25\") #make sure you are loading the right checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit = trained_model(input_ids, output_hidden_states=False).logits"
      ],
      "metadata": {
        "id": "jUOlXPmsP_KI"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(logittobinary(logit))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axzPgWRCRJ8L",
        "outputId": "6ab6801c-931f-4514-ebca-25b5d128f0cd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['00000000000000000000000000000']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This short test will likely be all zeros after training as the model will have at least fitted to the mean"
      ],
      "metadata": {
        "id": "-m2x21Mk3H0J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZaXPn-EGSNvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unused code I keep around for reference"
      ],
      "metadata": {
        "id": "_UiLC_I4ueUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create training data\n",
        "#input_sequences = df[\"IVS_SEQ\"].to_list()\n",
        "#input_labels = []\n",
        "\n",
        "#Sequences is a list of strings, input labels is a list of lists of token labels\n",
        "#for index, sample in df.iterrows():\n",
        "#    label_sequence = (sample[\"BP_POS\"]-sample[\"START\"])*[0] + [1] + (sample[\"END\"]-sample[\"BP_POS\"])*[0]\n",
        "#    input_labels.append(label_sequence)"
      ],
      "metadata": {
        "id": "pYZvdK2t_htF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use huggerface's official API to use SpliceBERT\n",
        "# get nucleotide embeddings (hidden states)\n",
        "#model = AutoModel.from_pretrained(SPLICEBERT_PATH) # load model\n",
        "#last_hidden_state = model(input_ids).last_hidden_state # get hidden states from last layer\n",
        "#hiddens_states = model(input_ids, output_hidden_states=True).hidden_states # hidden states from the embedding layer (nn.Embedding) and the 6 transformer encoder layers\n",
        "\n",
        "# get nucleotide type logits in masked language modeling\n",
        "#model = AutoModelForMaskedLM.from_pretrained(SPLICEBERT_PATH) # load model\n",
        "#logits = model(input_ids).logits # shape: (batch_size, sequence_length, vocab_size)"
      ],
      "metadata": {
        "id": "0Vi8nipsumAT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}