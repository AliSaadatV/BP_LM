{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11685,"status":"ok","timestamp":1732486393157,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"1cCX_QQAJuHn","outputId":"71d162e2-0c15-4b05-8fa4-52d1d8ff4674"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["# Install datasets as it is not already installed on colab\n","!pip install datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11420,"status":"ok","timestamp":1732486404575,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"LrxxMZWnSxnA"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, AutoModelForTokenClassification\n","from datasets import Dataset\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1732486404575,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"LSi8MpMXgs6S","outputId":"0516fcd8-ea19-4f52-beae-7819fd58c561"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{"id":"x8VPCc7SdQGG"},"source":["### Setup colab and load scripts from github"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89070,"status":"ok","timestamp":1732486493636,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"YBJNtW_XdceS","outputId":"ac17a7ad-e3be-4dda-cdc5-0cc9553b4822"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/epfl_ml_project\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Change working directory to Project folder, you may change this as needed\n","%cd \"/content/drive/MyDrive/epfl_ml_project\"\n","\n","#!git clone https://github.com/AliSaadatV/BP_LM.git clone repository if you haven't already\n","\n","from BP_LM.data_preprocessing import *"]},{"cell_type":"markdown","metadata":{"id":"wNlaTgOYWpj8"},"source":["### Load the dataset and prepare the labels and sequences"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20401,"status":"ok","timestamp":1732486514031,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"w7WNvUzx_bH6","outputId":"08a9d628-1cef-416c-b16c-39087000d093"},"outputs":[{"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/epfl_ml_project/BP_LM/data_preprocessing.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df['STRAND'] = df['STRAND'].replace({'+': 1, '-': 0})\n"]}],"source":["# I assume this directory contains two folders: data and models, containing the data and models respectively.\n","file_path = 'dataset/Data_GRCh38_canonical_transcript_intron_bp_database_Peng_20241113.txt'\n","\n","# Load dataset\n","df = pd.read_csv(file_path, sep='\\t')\n","\n","# Pick just a random subset to make the processing to come run fast.\n","# Remove this line to train on the full dataset\n","df = df.sample(n = 1000)\n","\n","df['BP_POS_WITHIN_STRAND'] = df['IVS_SIZE'] + df['BP_ACC_DIST']"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1732486514031,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"wu-RY0ExMbaA","outputId":"5ce71b34-4fb7-4620-8395-b07d55a75fcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chromosomes in train set: {'chr1', 'chr19', 'chr2', 'chr11', 'chr5', 'chr10', 'chrX', 'chr18', 'chr13', 'chr3', 'chr15', 'chr16', 'chr14', 'chr4', 'chr17', 'chr12', 'chrY', 'chr22'}\n","Chromosomes in validation set: {'chr9', 'chr7', 'chr21'}\n","Chromosomes in test set: {'chr20', 'chr8', 'chr6'}\n","\n","Total data points: 1000\n","Train set contains 788 data points (78.80%)\n","Validation set contains 104 data points (10.40%)\n","Test set contains 108 data points (10.80%)\n"]}],"source":["# Create a split based on chromosome types (Alis idea)\n","train_chrs = [\"chr1\", \"chr2\", \"chr3\", \"chr4\",\n","              \"chr5\",\"chr10\",\n","              \"chr11\", \"chr12\", \"chr13\", \"chr14\",\n","              \"chr15\", \"chr16\", \"chr17\", \"chr18\",\n","              \"chr19\", \"chr22\",\n","              \"chrX\", \"chrY\"]\n","\n","# Keep chr6 and chr7 in train if we want a 90/10/10 train/val/test split\n","test_chrs = [\"chr8\", \"chr20\", \"chr6\"]\n","val_chrs = [\"chr9\", \"chr21\", \"chr7\"]\n","\n","train_df, test_df, val_df = split_train_test_on_chr(df, train_chrs, val_chrs, test_chrs, shuffle=True)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1732486514031,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"rjO8wJHqgTRr"},"outputs":[],"source":["train_seqs, train_labels = extract_intron_seq_and_labels(train_df, max_model_input_size=1024, truncate=True)\n","test_seqs, test_labels = extract_intron_seq_and_labels(test_df, max_model_input_size=1024, truncate=True)\n","val_seqs, val_labels = extract_intron_seq_and_labels(val_df, max_model_input_size=1024, truncate=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1732486514031,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"CzYS6GWgRR1D","outputId":"7cfe935c-8643-4440-e5bb-2e895540f29b"},"outputs":[{"data":{"text/plain":["array(['CHR', 'START', 'END', 'STRAND', 'GENE', 'TRANSCRIPT', 'IVS',\n","       'IVS_SIZE', 'BP_ACC_DIST', 'IVS_SEQ', 'BP_POS_WITHIN_STRAND'],\n","      dtype=object)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df.columns.values"]},{"cell_type":"markdown","metadata":{"id":"Xzn3XsqAWx_Y"},"source":["### Load the splicebert model and tokenizer"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15496,"status":"ok","timestamp":1732486529522,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"I-5T13rQAbi0","outputId":"ca9de719-5a7f-4df1-8463-1d3f137f678c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at models/SpliceBERT.1024nt and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["SPLICEBERT_PATH = \"models/SpliceBERT.1024nt\"  # set the path to the folder of pre-trained SpliceBERT\n","\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(SPLICEBERT_PATH)\n","\n","# finetuning SpliceBERT for token classification tasks\n","model = AutoModelForTokenClassification.from_pretrained(SPLICEBERT_PATH, num_labels = 2) # We want binary classification on tokens so num_labels = 2"]},{"cell_type":"markdown","metadata":{"id":"so95_lpXW3al"},"source":["### Show a case of inference"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1732486529522,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"eL8Vked5UYkx","outputId":"ecc86e00-ef3d-490c-f6be-750c8a72fe5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[2, 6, 7, 6, 7, 6, 8, 9, 8, 9, 6, 8, 8, 6, 6, 8, 8, 6, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 3]\n"]}],"source":["# prepare a test input to show how to do inference\n","showcase_seq = \"ACGUACGuacguaCGuGCAUGUCAUGC\"\n","showcase_seq = \"acacaguguaggaaggaACGuacguaC\"  ## WARNING: this is just a demo. SpliceBERT may not work on sequences shorter than 64nt as it was trained on sequences of 64-1024nt in length\n","showcase_seq = ' '.join(list(showcase_seq.upper().replace(\"U\", \"T\"))) # U -> T and add whitespace\n","showcase_ids = tokenizer.encode(showcase_seq) # N -> 5, A -> 6, C -> 7, G -> 8, T(U) -> 9. NOTE: a [CLS] and a [SEP] token will be added to the start and the end of seq\n","print(showcase_ids) #Print the tokenized version\n","showcase_ids = torch.as_tensor(showcase_ids) # convert python list to Tensor\n","showcase_ids = showcase_ids.unsqueeze(0) # add batch dimension, shape: (batch_size, sequence_length)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":550,"status":"ok","timestamp":1732486530069,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"9Dd512BZUeQc"},"outputs":[],"source":["#infer with the (untrained) model\n","showcase_logit = model(showcase_ids, output_hidden_states=False).logits"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1732486530069,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"XvYheoTEN9Zw"},"outputs":[],"source":["#Define a function for making the output easily interpretable\n","def logittobinary(logits):\n","  \"\"\"\n","  This is just a ghetto function for converting binary logits into binary strings\n","  There might be an existing method for this in transformers, but I coulnd't find any\n","  so i wrote this real quick\n","  \"\"\"\n","  output_list = []\n","  for j in range(len(logits)):\n","    binary = \"\"\n","    for i in range(len(logits[j,:,1])):\n","      if logits[j,i,0] < logits[j,i,1]:\n","        binary += \"1\"\n","      else:\n","        binary += \"0\"\n","    output_list.append(binary)\n","  return output_list"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1732486530069,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"R8zvPbMoGuFK","outputId":"34acaf3d-ab4c-4563-885f-37b14e4c80d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[ 0.1932, -0.0200],\n","         [-0.0401, -0.0893],\n","         [ 0.2547, -0.0215],\n","         [-0.1226, -0.0999],\n","         [ 0.1999, -0.0790],\n","         [-0.1321, -0.0556],\n","         [ 0.1965, -0.2273],\n","         [ 0.2206,  0.1493],\n","         [ 0.0061, -0.1003],\n","         [ 0.2092,  0.1118],\n","         [-0.2125, -0.0328],\n","         [ 0.0986, -0.1546],\n","         [ 0.0882, -0.1458],\n","         [-0.1228, -0.1165],\n","         [-0.2095, -0.0359],\n","         [ 0.0291, -0.1297],\n","         [ 0.0239, -0.1281],\n","         [-0.1263, -0.0150],\n","         [-0.1821, -0.0824],\n","         [ 0.1641,  0.0965],\n","         [-0.0289, -0.0119],\n","         [ 0.0853, -0.0641],\n","         [-0.2176, -0.0215],\n","         [ 0.1937,  0.1328],\n","         [-0.0294, -0.0585],\n","         [ 0.2223, -0.1484],\n","         [-0.2186, -0.0195],\n","         [ 0.1952,  0.0893],\n","         [ 0.0634, -0.0005]]], grad_fn=<ViewBackward0>)\n","['00010100001001100110101000100']\n"]}],"source":["print(showcase_logit)\n","print(logittobinary(showcase_logit))"]},{"cell_type":"markdown","metadata":{"id":"LZ5tDwQxVArA"},"source":["The model is not yet finetuned so this output is just gibberish at the moment. Notice that for each token the likelyhood of a branchpoint is about the same as not a branchpoint (depends a bit on seed), which should definately not be the case once the model has been trained."]},{"cell_type":"markdown","metadata":{"id":"OCKbfRHsXHc1"},"source":["### Tokenize the training data and put it into the dataset format"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1732486530382,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"RkPF0Xq6AfLt"},"outputs":[],"source":["# Tokenize the input data\n","train_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in train_seqs] #There shouldn't be any \"U\"s in the training data, but I have kept the replacer line\n","test_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in test_seqs]\n","val_seqs = [' '.join(list(seq.upper().replace(\"U\", \"T\"))) for seq in val_seqs]"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1732486530383,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"g0bbaWmeQ5ZI","outputId":"2a7ee949-9259-42c5-9c68-849e8e76fab1"},"outputs":[{"name":"stdout","output_type":"stream","text":["['T T G T C A T T T A C C T G T A T T C C G T C A A A A T A A T T G T G T C A G G G T T T A C G A A A C A A C G A A T A T T A G C G G A A T T A A A A C G A A A T C C A A T A G T A T A T T G A G G T A C C A A G T C T T T C T G A C C G A C A C A C A G T T A C A G T T A G T G A C G G A T C C T G A T G A C T T C G T A C A C T C T T C G A C C G T T T A C T T T A T C G A T A T T G G G A C A G A T T C C C C T C C T T T C C T T C T C T A G G T A A A T G T G T A C A T C G T C T T G A G T C T A A T A T T T A A G A A C C T T C A G T C C T C G A C A C A G A T A T G A C C A A C G T G T T T C C T G A C A C C T T T A T G C G A A T A G T C A T T C G G T C C C A C A A C A A A G A T A A C A A T A C G T T T G G G G T G G A G A G G T C G T C A G A T A T T T A T C A T T A A C G A G A G G G A T T A T C C T C T G T G T A A T A C A A C C T T C T T G T T G T C T A T T A G A G A T A C G G T A C A C A T A G A T T C T T T T A A T T A C T T T T A A A T C T A A A C A A A A C T T T T A C A G T T G T T T T A T C T A T G T A A C A T G T A C A T G A A A A C T T T A G A G A A T G A G T A T T T A G A C G A C A G A G A A G A A A A G T T A A A A A C A G G T T C A A G A T T T A A C A C C T G G G G A G T G T T T T A G T A T A C A A C T T C G G G A T T G G A G G G A C A C T G A C A T A A A C T C C T A T C C G A G A T A T C C T C T A T T A A T T C C A A T T T A C T C A A T T T C T A C C A C G A C A T T A G G T T A T C C T G A C C C G G A A C A T T C T C C A C C G A A A C A T T C T T C T C T T T C T C T C T C T G T C T A G A G A G A G A C G T A T T T G T G G G T C C T T T C C G G T A C A C T C C T G T G T C A C T C T T C C A C C G G T G G A C G T T C A G T C C T T C T C T C G G G A G T G G T C T T T G G A T T A G C C A A C C T T G G A A C C T G A A A C A T C A G A G G T C T T A A C A C T C T T T T A T C T A A A G A C G G T A A A A T T C G G T G G G T G G G A C A C C A T A A A A C A C T A C C G T C G G A C T T G A C T G T T T C T G T C A G T A T G T G A C C T T T T G T T C T C G A A A T A T T T A T A T G T G T G T C A A A A C G A C C T C A G T G T G A G T G', 'G T A T G T C G A T C C C T G A G G G G C T T T C C T G C T T T G G A G G T C C T G G T G G T G G G A A T C A G G G T A C T C T G G G G C C G G G T A G G G A G A G A G G C T T C C T G T G G A T G G G C A G G G C G G G G C T C A G G A G G G G A A G G A G A G C T G G G C C C A C A C C C C T T C T C C T G A G G C T C A C C C C G G G A T T C C T G C G G C C A G T G C C T G G C C C C C T C C C C A G G T C C C T G C T C C T G T G G G C A G T G C C C G G T G C A G G C A G C C A C C T C T G G T A C C C G T T G C C T C T C C A G G C T C C T T T G C C C T G T A A C A T G G G A G G C C G A G G C T T G A T T A T A G A G T T G A T T T G A G G C T T A G C A G C A G A G T A T A G A A A G A C C T G G A A A A G C T C A G C T G T A G T A C C A G T C C C T C A G G T C C T G C A G C A C G G G A C C C T G C T T C A C G T G G A G G A C A T C T A C T T C A G C T G T T A T T T G A G G G C C A T G C T T C C C C C A T G T G G T C G G T C T G T A G G G A G C C A G G T G G G C C A T G G C T G T G C C A G T T G T T A A C G T T T A C T G A G T C C C T G G C A C T G A G C A G G T G C C C C A G T G C C C C G C C C A G T G C T G C A C T C C A T A G C T C A C T G C A T C C T T A C A C C A A C C C T G T G A G G C A A G G A T T A T T A A G G T C C C C A T T G A G T G G A A A G G G A G A C T A A T A C A G A G G G A G G G C A G G T G A C T T T C C C A G G G T A C A T G G A T T G T A G G T G G C A G A G C C C C T G G T C G A T C C T T T C C T G G G C T G G G T T A A C A G C A G C T C T G T G A C T T G A A G G T T C A C A T T C T C T G G G C C T C A A A G G C T G T G A C T G T A A C A T T C C A T T T C A G A C T G T C C C C A T G G G G A T G G G G G T G G G A G A A G G G A A A C A G C C G T T T C T C A T C C A C C C C C T T T G A A G T A G A G T C A A A A C T G T C T A G A G A G A G A A G G A A G C T G A C C C T T G C C T C T G C T A A T G G T T T C T G C T G C T T C C C C C C T C C T C C A G']\n"]}],"source":["print(train_seqs[:2])"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1430,"status":"ok","timestamp":1732486531811,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"7-EHbIu2AjhQ"},"outputs":[],"source":["# Package for use in dataset class\n","train_ids = tokenizer(train_seqs, padding = 'max_length', padding_side = 'left', max_length = 1024)\n","test_ids = tokenizer(test_seqs, padding = 'max_length', padding_side = 'left', max_length = 1024)\n","val_ids = tokenizer(val_seqs, padding = 'max_length', padding_side = 'left', max_length = 1024)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3267,"status":"ok","timestamp":1732486535077,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"2Q3ihysxI7Q6"},"outputs":[],"source":["# Build the dataset structure we will pass for training\n","train_dataset = Dataset.from_dict(train_ids)\n","train_dataset = train_dataset.add_column(\"labels\", train_labels)\n","\n","test_dataset = Dataset.from_dict(test_ids)\n","test_dataset = test_dataset.add_column(\"labels\", test_labels)\n","\n","val_dataset = Dataset.from_dict(val_ids)\n","val_dataset = val_dataset.add_column(\"labels\", val_labels)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3074,"status":"ok","timestamp":1732486538148,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"dI4LDqvNIT-m"},"outputs":[],"source":["# Set up the collator (I think it does padding)\n","from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer\n","data_collator = DataCollatorForTokenClassification(tokenizer)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1732486538148,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"I6zO7lLZI9wF"},"outputs":[],"source":["# Define model training parameters\n","model_name = SPLICEBERT_PATH.split(\"/\")[-1]\n","batch_size = 4\n","\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-secondary-structure\",\n","    eval_strategy = \"no\",\n","    save_strategy = \"epoch\",\n","    learning_rate=3e-4,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=1,\n","    weight_decay=0.001,\n","    #load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    #push_to_hub=True,\n",")"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1732486538148,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"6_uUngK4TkOA"},"outputs":[],"source":["# Do not save to W&B\n","import os\n","os.environ[\"WANDB_MODE\"] = \"disabled\""]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":68196,"status":"ok","timestamp":1732486606341,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"MOEm66ctT8dN","outputId":"2a2ee256-d065-4669-8f9d-ec8dcbedadaf"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-21-ffb7fee7e86c>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='197' max='197' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [197/197 01:02, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=197, training_loss=0.017273976718108666, metrics={'train_runtime': 66.1643, 'train_samples_per_second': 11.91, 'train_steps_per_second': 2.977, 'total_flos': 91758239336544.0, 'train_loss': 0.017273976718108666, 'epoch': 1.0})"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","trainer.train()\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":486,"status":"ok","timestamp":1732486635729,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"osaAl_hGLj56"},"outputs":[],"source":["trained_model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}-finetuned-secondary-structure/checkpoint-197\") #make sure you are loading the right checkpoint\n","trained_model = trained_model.to(device)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1732486636789,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"auEIuRbngSZk","outputId":"d5ffb8ff-d117-49d5-d6e9-05e9b9f25474"},"outputs":[{"name":"stdout","output_type":"stream","text":["['00000000000000000000000000000']\n"]}],"source":["showcase_ids = showcase_ids.to(device)\n","showcase_logit = model(showcase_ids, output_hidden_states=False).logits\n","print(logittobinary(showcase_logit))"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1732486637671,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"jUOlXPmsP_KI"},"outputs":[],"source":["logit = trained_model(showcase_ids, output_hidden_states=False).logits"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1732486639283,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"axzPgWRCRJ8L","outputId":"e454cdbf-acdd-472d-b4c3-7e8b02dbb0bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["['00000000000000000000000000000']\n"]}],"source":["print(logittobinary(logit))"]},{"cell_type":"markdown","metadata":{"id":"-m2x21Mk3H0J"},"source":["This short test will likely be all zeros after training as the model will have at least fitted to the mean"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1732486611472,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"ZaXPn-EGSNvX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_UiLC_I4ueUx"},"source":["### Unused code I keep around for reference"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1732486611472,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"pYZvdK2t_htF"},"outputs":[],"source":["#Create training data\n","#input_sequences = df[\"IVS_SEQ\"].to_list()\n","#input_labels = []\n","\n","#Sequences is a list of strings, input labels is a list of lists of token labels\n","#for index, sample in df.iterrows():\n","#    label_sequence = (sample[\"BP_POS\"]-sample[\"START\"])*[0] + [1] + (sample[\"END\"]-sample[\"BP_POS\"])*[0]\n","#    input_labels.append(label_sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1732486611472,"user":{"displayName":"Timothy Tran","userId":"07101945106214300698"},"user_tz":-60},"id":"0Vi8nipsumAT"},"outputs":[],"source":["# use huggerface's official API to use SpliceBERT\n","# get nucleotide embeddings (hidden states)\n","#model = AutoModel.from_pretrained(SPLICEBERT_PATH) # load model\n","#last_hidden_state = model(input_ids).last_hidden_state # get hidden states from last layer\n","#hiddens_states = model(input_ids, output_hidden_states=True).hidden_states # hidden states from the embedding layer (nn.Embedding) and the 6 transformer encoder layers\n","\n","# get nucleotide type logits in masked language modeling\n","#model = AutoModelForMaskedLM.from_pretrained(SPLICEBERT_PATH) # load model\n","#logits = model(input_ids).logits # shape: (batch_size, sequence_length, vocab_size)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}
