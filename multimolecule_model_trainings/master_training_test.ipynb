{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets evaluate multimolecule==0.0.5"
      ],
      "metadata": {
        "id": "pwz66zVBmFZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import (\n",
        "    DataCollatorForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import Dataset\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "lfMTN-dLmIRD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "WORKING_DIRECTORY = '/content/drive/MyDrive/Machine Learning (CS-433)/Project 2'\n",
        "DATASET_PATH = 'data/dataset.txt'\n",
        "\n",
        "%cd {WORKING_DIRECTORY}"
      ],
      "metadata": {
        "id": "W1DC2YMOmvib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from BP_LM.scripts.data_preprocessing import *\n",
        "from BP_LM.scripts.trainer_datasets_creation import create_dataset\n",
        "from BP_LM.scripts.compute_metrics import compute_metrics\n",
        "from BP_LM.scripts.model_choice import set_multimolecule_model\n",
        "\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "vGE2uoUlm0_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and dataset variables\n",
        "\n",
        "MULTIMOLECULE_MODEL = \"splicebert\" # Available models are: rnafm, rnamsm, ernierna, utrlm-te_el, splicebert, rnabert.\n",
        "SAMPLE_N_DATAPOINTS = 100  # Set to None to use the full dataset\n",
        "SEED = 32\n",
        "\n",
        "# Training hyperparameters\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 3e-4\n",
        "TRAIN_EPOCHS = 3\n",
        "WEIGHT_DECAY = 0.001\n",
        "OPTIMIZER = \"adamw_torch\"\n",
        "MODEL_OUTPUT_DIRECTORY = f\"multimolecule-{MULTIMOLECULE_MODEL}-finetuned-secondary-structure\""
      ],
      "metadata": {
        "id": "zIXP7MqImMs4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the selected multimolecule model\n",
        "model, tokenizer, MODEL_MAX_INPUT_SIZE = set_multimolecule_model(MULTIMOLECULE_MODEL)\n",
        "\n",
        "# Load data and create dataset\n",
        "df = pd.read_csv(DATASET_PATH, sep='\\t')\n",
        "train_dataset, val_dataset, test_dataset = create_dataset(df, tokenizer, model, MODEL_MAX_INPUT_SIZE, SEED, SAMPLE_N_DATAPOINTS)\n",
        "\n",
        "# Set up data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "mbd5Mf_4mfVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=MODEL_OUTPUT_DIRECTORY,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=TRAIN_EPOCHS,\n",
        "    optim=OPTIMIZER,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"F1\",\n",
        "    # eval_accumulation_steps=10,\n",
        ")\n",
        "\n",
        "#Define metrics function\n",
        "metrics = lambda x: compute_metrics(x, \"test_metrics\")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "qj_u1q5Vne7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}